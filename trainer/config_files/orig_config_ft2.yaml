number: '0123456789'
symbol: "!#$%&()*+,-.:;<=>?@[]^{|}~ "
lang_char: 'ABCDEFGHIJKLMNOPQRSTUVWXYZАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'
experiment_name: 'ru_orig_ft'
train_data: 'all_data'
valid_data: False
#valid_data: 'all_data/ocr_doc_dataset_all'
#Text generator options tg_*
tg_blur: 2
tg_random_blur: True
tg_words_len: 2
#tg_distributed: "172.25.0.86:50051"
tg_distributed: False
manualSeed: 1111
workers: 8
batch_size: 128 #32
num_iter: 500000
valInterval: 1000
saved_model: 'saved_models/ru_orig_ft/iter_80000.pth' #'saved_models/cyrillic_g2.pth'
FT: True
optim: 'adamw' # default is Adadelta
lr: 0.1
beta1: 0.9
rho: 0.9
eps: 0.00000001
grad_clip: 5
#Data processing
#select_data: 'text_dataset_ru_gray_10k3' # this is dataset folder in train_data
select_data: False # this is dataset folder in train_data
batch_ratio: '1' 
total_data_usage_ratio: 1.0
batch_max_length: 48
imgH: 64
imgW: 1200
rgb: False
contrast_adjust: False
sensitive: False
PAD: True
contrast_adjust: 0.0
data_filtering_off: True
# Model Architecture
Transformation: 'None'
FeatureExtraction: 'VGG'
SequenceModeling: 'BiLSTM'
Prediction: 'Attn'
num_fiducial: 20
input_channel: 1
output_channel: 256
hidden_size: 256
decode: 'greedy'
new_prediction: False
freeze_FeatureFxtraction: False
freeze_SequenceModeling: False
freeze_Prediction: False
